{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Qwen2.5-Omni","metadata":{}},{"cell_type":"markdown","source":"## Packages and dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n!pip install accelerate\n# It's highly recommended to use `[decord]` feature for faster video loading.\n!pip install qwen-omni-utils[decord] -U\n!pip install accelerate\n!pip install ipywidgets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/DrCet/multimodal-model-inference-and-finetuning","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    HfArgumentParser,\n    Qwen2_5OmniForConditionalGeneration,\n    Qwen2_5OmniProcessor\n)\nfrom qwen_omni_utils import process_mm_info\nimport sys \nimport os\nimport re\nimport soundfile as sf \nfrom IPython.display import Audio, display, clear_output\nimport time\nimport ipywidgets as widgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:13:39.936818Z","iopub.execute_input":"2025-05-18T05:13:39.937539Z","iopub.status.idle":"2025-05-18T05:13:53.095244Z","shell.execute_reply.started":"2025-05-18T05:13:39.937509Z","shell.execute_reply":"2025-05-18T05:13:53.094671Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 05:13:48.699904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747545228.723181     211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747545228.730302     211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## is_flash_attention_2_supported","metadata":{}},{"cell_type":"code","source":"def is_flash_attention_2_supported():\n    try:\n        # Check CUDA availability and compute capability\n        if not torch.cuda.is_available():\n            print(\"CUDA not available.\")\n            return False\n        compute_capability = torch.cuda.get_device_properties(0).major * 10 + torch.cuda.get_device_properties(0).minor\n        if compute_capability < 80:  # Need compute capability >= 8.0\n            print(f\"GPU compute capability {compute_capability/10} is not supported (requires >= 8.0).\")\n            return False\n\n        # Check PyTorch and CUDA versions\n        torch_version = torch.__version__.split(\"+\")[0]\n        torch_major, torch_minor = map(int, torch_version.split(\".\")[:2])\n        cuda_version = torch.version.cuda\n        cuda_major, cuda_minor = map(int, cuda_version.split(\".\")[:2]) if cuda_version else (0, 0)\n        if torch_major < 2 or (torch_major == 2 and torch_minor < 2):\n            print(f\"PyTorch {torch_version} is not supported (requires >= 2.2).\")\n            return False\n        if cuda_major < 11 or (cuda_major == 11 and cuda_minor < 7):\n            print(f\"CUDA {cuda_version} is not supported (requires >= 11.7).\")\n            return False\n\n        # Check Flash Attention availability\n        if not torch.backends.cuda.flash_sdp_enabled():\n            print(\"FlashAttention-2 is not available. Ensure flash-attn >= 2.1.0 is installed.\")\n            return False\n\n        print(\"FlashAttention-2 is supported.\")\n        return True\n    except Exception as e:\n        print(f\"Error checking FlashAttention-2 compatibility: {e}\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:13:53.096569Z","iopub.execute_input":"2025-05-18T05:13:53.097390Z","iopub.status.idle":"2025-05-18T05:13:53.103942Z","shell.execute_reply.started":"2025-05-18T05:13:53.097365Z","shell.execute_reply":"2025-05-18T05:13:53.103382Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## args","metadata":{}},{"cell_type":"code","source":"model_name_or_path = \"Qwen/Qwen2.5-Omni-3B\"\ndevice_map = 'auto'\ntorch_dtype = 'auto'\nweights_only = False\nattn_implementation = \"flash_attention_2\" if is_flash_attention_2_supported() else \"sdpa\"\nspeaker = 'Ethan' #Chelsie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:52:02.050260Z","iopub.execute_input":"2025-05-18T05:52:02.050831Z","iopub.status.idle":"2025-05-18T05:52:02.055509Z","shell.execute_reply.started":"2025-05-18T05:52:02.050804Z","shell.execute_reply":"2025-05-18T05:52:02.054614Z"}},"outputs":[{"name":"stdout","text":"GPU compute capability 7.5 is not supported (requires >= 8.0).\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nfor i in range(torch.cuda.device_count()):\n    with torch.cuda.device(i):\n        torch.cuda.empty_cache()\n\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    model_name_or_path,\n    device_map=device_map,\n    torch_dtype=torch_dtype,\n    attn_implementation=attn_implementation,\n    weights_only=weights_only\n)\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(model_name_or_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:13:53.143879Z","iopub.execute_input":"2025-05-18T05:13:53.144670Z","iopub.status.idle":"2025-05-18T05:14:05.077971Z","shell.execute_reply.started":"2025-05-18T05:13:53.144641Z","shell.execute_reply":"2025-05-18T05:14:05.077378Z"}},"outputs":[{"name":"stderr","text":"Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40af0213cb04f6bafc68f1f198ab413"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:4379: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  for key, value in torch.load(path).items():\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Preprocess","metadata":{}},{"cell_type":"code","source":"add_generation_prompt = True \ntokenize = False\nuse_audio_in_video = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:14:05.078869Z","iopub.execute_input":"2025-05-18T05:14:05.079149Z","iopub.status.idle":"2025-05-18T05:14:05.082986Z","shell.execute_reply.started":"2025-05-18T05:14:05.079127Z","shell.execute_reply":"2025-05-18T05:14:05.082407Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\ndef extract_prompt_elements(prompt, verbose=False):\n    \"\"\"Extract text, image, audio, and video from prompt.\"\"\"\n    # Regex patterns\n    image_pattern = re.compile(r'((https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+|[^\\s<>\"]+\\.(jpg|jpeg|png|gif|bmp))($|[^\\w]))')\n    audio_pattern = re.compile(r'((https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+|[^\\s<>\"]+\\.(wav|mp3|ogg|aac|flac))($|[^\\w]))')\n    video_pattern = re.compile(r'((https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+|[^\\s<>\"]+\\.(mp4|avi|mov|wmv|mkv))($|[^\\w]))')\n    \n    elements = {\n        \"images\": [],\n        \"audio\": [],\n        \"video\": [],\n        \"text\": prompt\n    }\n    \n    # Extract URLs and files\n    for pattern, key in [\n        (video_pattern, \"video\"),\n        (audio_pattern, \"audio\"),\n        (image_pattern, \"images\"),\n    ]:\n        matches = pattern.findall(prompt)\n        for match in matches:\n            url_or_path = match[0]  # Full match (without boundary)\n            elements[key].append(url_or_path)\n            # Remove from prompt to isolate text\n            elements[\"text\"] = re.sub(pattern, ' ', elements[\"text\"])\n    \n    # Clean up text (remove extra spaces)\n    elements[\"text\"] = ' '.join(elements[\"text\"].split())\n    # if verbose:\n    #     print(\"Extracted elements:\", elements)\n    \n    return elements","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:14:05.083821Z","iopub.execute_input":"2025-05-18T05:14:05.084093Z","iopub.status.idle":"2025-05-18T05:14:05.106136Z","shell.execute_reply.started":"2025-05-18T05:14:05.084068Z","shell.execute_reply":"2025-05-18T05:14:05.105575Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def is_image_file(path):\n    \"\"\"Check if path is an image file.\"\"\"\n    try:\n        return path.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))\n    except:\n        return False\n\ndef is_audio_file(path):\n    \"\"\"Check if path is an audio file.\"\"\"\n    try:\n        return path.lower().endswith(('.wav', '.mp3', '.ogg', '.aac', '.flac'))\n    except:\n        return False\n\ndef is_video_file(path):\n    \"\"\"Check if path is a video file.\"\"\"\n    try:\n        return path.lower().endswith(('.mp4', '.avi', '.mov', '.wmv', '.mkv'))\n    except:\n        return False\ndef prepare_inputs(conversation=None, elements=None):\n        prompt_template = {\n            'role':'user',\n            'content':[\n                {'type':'text', 'text':elements['text']}\n            ]\n        }\n\n        # Add images to prompt\n        for image in elements['images']:\n            if is_image_file(image):\n                prompt_template['content'].append({'type':'image', 'image':image})\n\n        # Add audio to prompt\n        for audio in elements['audio']:\n            if is_audio_file(audio):\n                prompt_template['content'].append({'type':'audio', 'audio':audio})\n\n        # Add video to prompt\n        for video in elements['video']:\n            if is_video_file(video):\n                prompt_template['content'].append({'type':'video', 'image':video})\n        \n        conversation.append(prompt_template)\n        text = processor.apply_chat_template(conversation, add_generation_prompt=add_generation_prompt, tokenize=tokenize)\n        audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_audio_in_video)\n        inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=use_audio_in_video)\n        # Move inputs to model dtype and device\n        inputs = inputs.to(model.device).to(model.dtype)\n        return conversation, inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:14:05.106889Z","iopub.execute_input":"2025-05-18T05:14:05.107172Z","iopub.status.idle":"2025-05-18T05:14:05.134927Z","shell.execute_reply.started":"2025-05-18T05:14:05.107149Z","shell.execute_reply":"2025-05-18T05:14:05.134225Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Define chat","metadata":{}},{"cell_type":"code","source":"def chat():\n    conversation = [\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n                ]\n            }\n        ]\n    # Create chat interface\n    prompt_widget = widgets.Text(\n        value='',\n        placeholder='Type your prompt (e.g., Describe image1.jpg and image2.jpg)',\n        description='You:',\n        layout={'width': '500px'}\n    )\n    submit_button = widgets.Button(\n        description='Submit',\n        button_style='primary',\n        tooltip='Click to submit prompt'\n    )\n\n    image_upload = widgets.FileUpload(\n        accept='.jpg,.jpeg,.png,.gif,.bmp',\n        multiple=False,\n        description='Image'\n    )\n\n    video_upload = widgets.FileUpload(\n        accept='.mp4',\n        multiple=False,\n        description='Video'\n    )\n    output = widgets.Output()\n\n    def on_submit(button):\n        nonlocal conversation\n        with output:\n            prompt = prompt_widget.value.strip()\n            if prompt.lower() == 'exit':\n                print(\"Chat ended.\")\n                return\n\n            elements = extract_prompt_elements(prompt, verbose=True)\n            conversation, inputs = prepare_inputs(conversation, elements)\n\n            try:\n                text_ids, audio = model.generate(**inputs, use_audio_in_video=use_audio_in_video, speaker=speaker)\n            except Exception as e:\n                print(f\"Inference failed: {e}\")\n                return\n\n            text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n            response = text[0].split('assistant\\n')[-1]\n            conversation.append({\n                'role':'system',\n                'content':[{'type':'text','text':response}]\n            })\n            \n            os.makedirs('.generated_audio', exist_ok=True)\n            output_audio = \".generated_audio/response.wav\"\n            if audio is not None:\n                sf.write(\n                    output_audio,\n                    audio.reshape(-1).detach().cpu().numpy(),\n                    samplerate=24000\n                )\n\n            if audio is not None:\n                audio_data, sample_rate = sf.read(output_audio)\n                audio_duration = len(audio_data) / sample_rate\n                display(Audio(output_audio, autoplay=True))\n                time.sleep(audio_duration + 0.5)\n            prompt_widget.value = ''  # Clear input after submission\n\n    submit_button.on_click(on_submit)\n    display(widgets.VBox([image_upload, video_upload,prompt_widget, submit_button, output]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:52:31.974454Z","iopub.execute_input":"2025-05-18T05:52:31.974744Z","iopub.status.idle":"2025-05-18T05:52:31.983958Z","shell.execute_reply.started":"2025-05-18T05:52:31.974723Z","shell.execute_reply":"2025-05-18T05:52:31.983229Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:52:35.885169Z","iopub.execute_input":"2025-05-18T05:52:35.885457Z","iopub.status.idle":"2025-05-18T05:52:35.902381Z","shell.execute_reply.started":"2025-05-18T05:52:35.885439Z","shell.execute_reply":"2025-05-18T05:52:35.901506Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(FileUpload(value=(), accept='.jpg,.jpeg,.png,.gif,.bmp', description='Image'), FileUpload(value…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a606183269c74420a2c2f9f366671d6f"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}