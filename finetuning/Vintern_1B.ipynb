{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\nmodel_name = \"5CD-AI/Vintern-1B-v3_5\"\n\nmodel = AutoModel.from_pretrained(\n    model_name,\n    trust_remote_code=True\n).cuda()\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False, padding='right')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:44:44.785964Z","iopub.execute_input":"2025-05-23T05:44:44.786626Z","iopub.status.idle":"2025-05-23T05:44:47.025782Z","shell.execute_reply.started":"2025-05-23T05:44:44.786599Z","shell.execute_reply":"2025-05-23T05:44:47.025187Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('5CD-AI/Vietnamese-Multi-turn-Chat-Alpaca')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:32:05.060278Z","iopub.execute_input":"2025-05-23T05:32:05.060973Z","iopub.status.idle":"2025-05-23T05:32:05.727150Z","shell.execute_reply.started":"2025-05-23T05:32:05.060946Z","shell.execute_reply":"2025-05-23T05:32:05.726640Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def prepare_dataset(sample):\n    processed_sample = []\n    for turn in sample['conversations']:\n        processed_turn = {}\n        if turn['from'] == 'human':\n            processed_turn['role'] = 'user'\n        elif turn['from'] == 'gpt':\n            processed_turn['role'] = 'assistant'\n        processed_turn['content'] = turn['value']\n        processed_sample.append(processed_turn)\n    return {'conversations':processed_sample}\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:32:05.728319Z","iopub.execute_input":"2025-05-23T05:32:05.728580Z","iopub.status.idle":"2025-05-23T05:32:05.732595Z","shell.execute_reply.started":"2025-05-23T05:32:05.728553Z","shell.execute_reply":"2025-05-23T05:32:05.732058Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Apply dataset preparation\nprint(\"Preparing dataset...\")\ndataset = dataset['train'].map(prepare_dataset, remove_columns=['conversations', 'id'], num_proc=4)\nprint(f\"Processed dataset sample: {dataset[0]['conversations']}\")\n\n# Apply chat template (no pixel_values here)\nprint(\"Applying chat template...\")\ndataset = dataset.map(lambda x: {'text': tokenizer.apply_chat_template(x['conversations'], tokenize=False)}, num_proc=4, remove_columns=['conversations'])\nprint(f\"Formatted sample: {dataset[0]['text'][:200]}...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:32:08.140481Z","iopub.execute_input":"2025-05-23T05:32:08.140771Z","iopub.status.idle":"2025-05-23T05:32:12.553127Z","shell.execute_reply.started":"2025-05-23T05:32:08.140751Z","shell.execute_reply":"2025-05-23T05:32:12.552225Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset...\nProcessed dataset sample: [{'content': 'Hãy chỉnh sửa câu này để ngắn gọn hơn mà không mất đi ý nghĩa: \"Trận đấu là một thất bại nặng nề mặc dù thực tế là cả đội đã tập luyện trong nhiều tuần.\"', 'role': 'user'}, {'content': 'Nhiều tuần huấn luyện của đội đã dẫn đến một thất bại nặng nề.', 'role': 'assistant'}, {'content': 'Bạn có thể đề xuất một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của họ trong trận đấu tiếp theo không?', 'role': 'user'}, {'content': 'Chắc chắn, đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của mình trong trận đấu tiếp theo: 1. Phân tích trận đấu trước bằng cách xem lại cảnh quay trận đấu để xác định điểm yếu và các lĩnh vực cần cải thiện. 2. Tăng cường độ và sự tập trung của các buổi tập để đảm bảo mọi cầu thủ đều được chuẩn bị đầy đủ về thể chất và tinh thần. 3. Luyện tập những kỹ năng cụ thể cần cải thiện, chẳng hạn như sút bóng hoặc chuyền bóng chính xác. 4. Phát triển các chiến lược và chiến thuật của đội có tính đến điểm mạnh và điểm yếu của cả đội bạn và đối thủ. 5. Xây dựng sự gắn kết và giao tiếp trong nhóm để cải thiện sự cộng tác và làm việc nhóm trên thực địa. 6. Phát triển tinh thần dẻo dai và khả năng phục hồi để vượt qua thất bại và duy trì sự tập trung trong các tình huống áp lực cao. 7. Tăng cường thể chất để cải thiện sức bền và giảm mệt mỏi khi chơi game. 8. Nghỉ ngơi và phục hồi - đảm bảo rằng các cầu thủ được nghỉ ngơi đầy đủ và dinh dưỡng đầy đủ trước trận đấu để đảm bảo mọi người đều ở trạng thái tốt nhất.', 'role': 'assistant'}, {'content': 'Cảm ơn những lời đề nghị. Bạn nghĩ chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm?', 'role': 'user'}, {'content': 'Là một mô hình ngôn ngữ AI, tôi không thể dự đoán chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm vì mỗi nhóm là duy nhất và có thể yêu cầu các cách tiếp cận khác nhau để cải thiện hiệu suất của họ. Tuy nhiên, tôi có thể đề xuất rằng đội nên ưu tiên các chiến lược giải quyết những điểm yếu nhất của họ, đảm bảo rằng các cầu thủ của họ ở trạng thái thể chất và tinh thần tốt nhất, đồng thời xây dựng tinh thần đồng đội gắn kết để tận dụng tối đa mọi cầu thủ.', 'role': 'assistant'}, {'content': 'Đó là một điểm tuyệt vời. Làm thế nào đội có thể cải thiện khả năng giao tiếp trên sân?', 'role': 'user'}, {'content': 'Cải thiện giao tiếp trên thực địa là rất quan trọng để xây dựng một nhóm hiệu quả có thể làm việc cùng nhau và đạt được thành công. Dưới đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện khả năng giao tiếp của họ trên hiện trường: 1. Thiết lập các giao thức liên lạc rõ ràng - thiết lập các tín hiệu và mã rõ ràng để mọi người trên hiện trường hiểu ý nghĩa của từng tín hiệu. 2. Khuyến khích giao tiếp cởi mở - người chơi nên được khuyến khích giao tiếp với nhau một cách tự do và cởi mở, không sợ bị chỉ trích hay bị trừng phạt. 3. Lặp lại thông tin chính - thông tin quan trọng, chẳng hạn như vị trí của người chơi, nên được lặp lại nhiều lần để đảm bảo rằng mọi người đều hiểu được. 4. Phát triển niềm tin - Người chơi nên tin tưởng lẫn nhau để đưa ra quyết định đúng đắn và giao tiếp hiệu quả, điều này sẽ giảm bớt sự nhầm lẫn và hiểu lầm trong quá trình chơi game. 5. Sử dụng ngôn ngữ tích cực - Ngôn ngữ tích cực có thể giúp thúc đẩy một môi trường hợp tác, hỗ trợ, khuyến khích giao tiếp và làm việc theo nhóm. 6. Thực hành giao tiếp trong quá trình tập luyện - Giao tiếp nên là một phần của các buổi tập luyện thường xuyên, để các cầu thủ quen với việc giao tiếp với nhau trong các trận đấu. Bằng cách tuân theo các chiến lược này, nhóm có thể cải thiện khả năng giao tiếp trên thực địa và làm việc cùng nhau hiệu quả hơn để đạt được mục tiêu của mình.', 'role': 'assistant'}, {'content': 'Cảm ơn đã phản ứng chi tiết. Làm thế nào nhóm có thể theo dõi tiến trình của họ và đảm bảo rằng họ đang cải thiện theo thời gian?', 'role': 'user'}, {'content': 'Đo lường tiến độ và phân tích kết quả có thể giúp nhóm đi đúng hướng và thực hiện các điều chỉnh để cải thiện hiệu suất của họ. Dưới đây là một số cách mà nhóm có thể theo dõi tiến trình của mình theo thời gian: 1. Đặt ra các mục tiêu cụ thể cần đạt được - việc có những mục tiêu rõ ràng và có thể đo lường được mà mọi người trong nhóm đều hiểu có thể giúp họ luôn có động lực và tập trung. 2. Thu thập dữ liệu và số liệu thống kê - dữ liệu có thể cung cấp phản hồi có giá trị về hiệu suất của đội, chẳng hạn như số bàn thắng ghi được, đường kiến \\u200b\\u200btạo và số cú sút trúng đích. 3. Sử dụng đánh giá của người chơi - đánh giá hiệu suất định kỳ có thể giúp xác định các lĩnh vực cần cải thiện và các lĩnh vực đang hoạt động tốt. 4. Phân tích cảnh quay trò chơi - sử dụng cảnh quay trò chơi để phân tích chuyển động, kiểu mẫu và giao tiếp của người chơi, nhóm có thể xác định các lĩnh vực cần cải thiện. 5. Theo dõi mức độ thể lực - theo dõi mức độ thể lực theo thời gian có thể giúp nhóm theo dõi sự cải thiện về thể lực, điều này có thể dẫn đến hiệu suất tốt hơn trên sân. 6. Kết hợp phản hồi từ huấn luyện viên và đồng đội - phản hồi từ huấn luyện viên và đồng đội có thể cung cấp những hiểu biết sâu sắc và ý tưởng có giá trị giúp cải thiện hiệu suất. Bằng cách liên tục theo dõi tiến trình của họ, nhóm có thể đảm bảo rằng họ đang tiến bộ theo thời gian và thực hiện các điều chỉnh về chiến lược cũng như đào tạo nếu cần thiết.', 'role': 'assistant'}]\nApplying chat template...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/12697 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f7e7c79e3414d589421626862c32059"}},"metadata":{}},{"name":"stdout","text":"Formatted sample: <|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHãy chỉnh sửa câu này để ngắn gọn hơn mà không mất đi ý nghĩa: \"Trận đấu là một thất ...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def tokenize_dataset(sample):\n    return tokenizer(sample['text'], return_tensors=None)\n\ndataset = dataset.map(tokenize_dataset, remove_columns=['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:39:07.013944Z","iopub.execute_input":"2025-05-23T05:39:07.014670Z","iopub.status.idle":"2025-05-23T05:40:28.378285Z","shell.execute_reply.started":"2025-05-23T05:39:07.014631Z","shell.execute_reply":"2025-05-23T05:40:28.377550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12697 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7ee49837f14dd390c5b1c44353d08f"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2035 > 1700). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\n# Custom data collator to add dummy pixel_values\nclass CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    def __call__(self, examples):\n        texts = [ex['input_ids'] for ex in examples]\n        encodings = self.tokenizer.pad(\n            {'input_ids':texts},\n            padding=True,\n            max_length=512,\n            return_tensors='pt'\n        )\n        batch = {\n            'input_ids': encodings['input_ids'],\n            'attention_mask': encodings['attention_mask'],\n        }\n\n        batch_size = len(examples)\n        batch['pixel_values'] = torch.zeros((batch_size, 3, 448, 448), dtype=torch.float16)\n        batch['image_flags'] = torch.tensor((batch_size,0), dtype=torch.int)\n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:43:18.647231Z","iopub.execute_input":"2025-05-23T05:43:18.647800Z","iopub.status.idle":"2025-05-23T05:43:18.652882Z","shell.execute_reply.started":"2025-05-23T05:43:18.647780Z","shell.execute_reply":"2025-05-23T05:43:18.652183Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport os\nfrom torch.utils.data import DataLoader\n\n# Disable W&B to avoid hangs\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen2_finetuned\",\n    per_device_train_batch_size=1,  # Minimize memory usage\n    gradient_accumulation_steps=8,  # Effective batch size = 8\n    learning_rate=2e-5,\n    num_train_epochs=1,  # Start with 1 epoch\n    logging_steps=1,  # Log every step\n    logging_dir=\"./logs\",\n    fp16=True,  # Mixed precision\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    report_to=\"none\",  # Disable all logging integrations\n)\n\nclass CustomTrainer(Trainer):\n    def get_train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.args.per_device_train_batch_size,\n            collate_fn=self.data_collator,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n        )\n    def get_eval_dataloader(self, eval_dataset=None):\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        return DataLoader(\n            eval_dataset,\n            batch_size=self.args.per_device_eval_batch_size,\n            collate_fn=self.data_collator,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            )\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=CustomDataCollator(tokenizer=tokenizer),\n)\n\n# Start fine-tuning\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:44:58.731985Z","iopub.execute_input":"2025-05-23T05:44:58.732481Z","iopub.status.idle":"2025-05-23T05:44:59.487443Z","shell.execute_reply.started":"2025-05-23T05:44:58.732462Z","shell.execute_reply":"2025-05-23T05:44:59.486435Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3442276110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_internvl_chat.py\", line 107, in forward\n    vit_embeds = self.extract_feature(pixel_values)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_internvl_chat.py\", line 185, in extract_feature\n    vit_embeds = self.vision_model(\n                 ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_intern_vit.py\", line 411, in forward\n    hidden_states = self.embeddings(pixel_values)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_intern_vit.py\", line 164, in forward\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, channel, width, height]\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_internvl_chat.py\", line 107, in forward\n    vit_embeds = self.extract_feature(pixel_values)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_internvl_chat.py\", line 185, in extract_feature\n    vit_embeds = self.vision_model(\n                 ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_intern_vit.py\", line 411, in forward\n    hidden_states = self.embeddings(pixel_values)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-1B-v3_5/115975aca0407d3c54bb0beb8a1da54d31de3f20/modeling_intern_vit.py\", line 164, in forward\n    patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, channel, width, height]\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\n","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"help(model.forward)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T04:28:16.181340Z","iopub.execute_input":"2025-05-22T04:28:16.181630Z","iopub.status.idle":"2025-05-22T04:28:16.186680Z","shell.execute_reply.started":"2025-05-22T04:28:16.181611Z","shell.execute_reply":"2025-05-22T04:28:16.185702Z"}},"outputs":[{"name":"stdout","text":"Help on method forward in module transformers_modules.5CD-AI.Vintern-1B-v3_5.115975aca0407d3c54bb0beb8a1da54d31de3f20.modeling_internvl_chat:\n\nforward(pixel_values: torch.FloatTensor, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, image_flags: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple, transformers.modeling_outputs.CausalLMOutputWithPast] method of transformers_modules.5CD-AI.Vintern-1B-v3_5.115975aca0407d3c54bb0beb8a1da54d31de3f20.modeling_internvl_chat.InternVLChatModel instance\n    Define the computation performed at every call.\n    \n    Should be overridden by all subclasses.\n    \n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        registered hooks while the latter silently ignores them.\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}